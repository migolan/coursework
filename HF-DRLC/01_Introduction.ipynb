{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migolan/RL-notebooks/blob/main/HF_RL_unit1_distilled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "This notebook is based on https://huggingface.co/learn/deep-rl-course.\n",
        "* Create the [lunar lander](https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4) environment using the gymnasium package\n",
        "* Train a PPO RL agent on the environment, using the stable_baselines3 package\n",
        "* Upload & download models to the HF models hub"
      ],
      "metadata": {
        "id": "YWE_n_MUTMYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "7z5ptC3DGYhO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "outputs": [],
      "source": [
        "!apt install swig cmake\n",
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip install --upgrade ipykernel\n",
        "import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "J-SOU0VdGc3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BE5JWP5rQIKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb8e1b1-6379-4cdb-f522-eaf64ce67816"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7c32112b7c40>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# RL environments library\n",
        "import gymnasium as gym\n",
        "\n",
        "# RL agents library https://stable-baselines3.readthedocs.io/en/master/\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub # upload and download trained models from the hub\n",
        "# Deep reinforcement Learning models available are listed at https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
        "\n",
        "# for visualization\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the LunarLander-v2 environment"
      ],
      "metadata": {
        "id": "yAXkRebv9N6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create environment"
      ],
      "metadata": {
        "id": "dUtL7c_gKFe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"LunarLander-v2\"  # https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "env = gym.make(env_id)\n",
        "observation, info = env.reset()"
      ],
      "metadata": {
        "id": "QgiusFgJKI37"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "State and action spaces"
      ],
      "metadata": {
        "id": "l37PsjSDDqcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation Space Shape:\", env.observation_space.shape)\n",
        "print(\"Observation Space Sample:\", env.observation_space.sample()) # Get a random observation\n",
        "print(\"Action Space Shape:\", env.action_space.n)\n",
        "print(\"Action Space Sample:\", env.action_space.sample()) # sample a random action"
      ],
      "metadata": {
        "id": "FpGPOhtx9cYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb1c868-b156-4eef-c9fd-db1724e12ad4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space Shape: (8,)\n",
            "Observation Space Sample: [15.688143   35.103764   -3.477983   -3.7590947   1.2247837  -4.8451624\n",
            "  0.81280357  0.29216114]\n",
            "Action Space Shape: 4\n",
            "Action Space Sample: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment rollout"
      ],
      "metadata": {
        "id": "R_IsuaCnES8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w7vOFlpA_ONz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f6ddc3-4da3-41de-9176-819b0d67c9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: 2\n",
            "Observation: [ 0.00320358  1.3872495   0.15871033 -0.5169701  -0.004113   -0.04441812\n",
            "  0.          0.        ]\n",
            "Reward: 2.583969659896792\n",
            "Action taken: 3\n",
            "Observation: [ 0.0048398   1.3750271   0.16767475 -0.5432326  -0.00812759 -0.08029964\n",
            "  0.          0.        ]\n",
            "Reward: -1.9834527836584346\n",
            "Action taken: 3\n",
            "Observation: [ 0.00655556  1.3621925   0.17766216 -0.5704695  -0.01414298 -0.12031905\n",
            "  0.          0.        ]\n",
            "Reward: -2.246094417031627\n",
            "Action taken: 1\n",
            "Observation: [ 0.00819874  1.3487482   0.1685482  -0.59757644 -0.01832837 -0.08371522\n",
            "  0.          0.        ]\n",
            "Reward: -1.4447394261140107\n",
            "Action taken: 0\n",
            "Observation: [ 0.00984211  1.3347039   0.16856004 -0.62424624 -0.02251434 -0.08372692\n",
            "  0.          0.        ]\n",
            "Reward: -1.5865053614883777\n"
          ]
        }
      ],
      "source": [
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(5):\n",
        "  # sample a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # perform action and observe state and reward\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  # https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "  print(f\"Observation: {observation}\")\n",
        "  print(f\"Reward: {reward}\")\n",
        "  if info:\n",
        "    print(f\"Info: {info}\")\n",
        "\n",
        "  if terminated or truncated:\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "**Lunar lander envirnoment**\n",
        "\n",
        "**Observation** is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n",
        "\n",
        "**The action space** (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "**Reward function** (the function that will give a reward at each timestep) üí∞:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train an RL agent on the environment"
      ],
      "metadata": {
        "id": "pVzj4l9h9nuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vectorized environment - stack multiple independent environments into\n",
        "# a single environment, to have more diverse experiences during the training.\n",
        "env = make_vec_env(env_id, n_envs=16)\n",
        "\n",
        "# Generate an agent with PPO learning algorithm\n",
        "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
        "# if we had frames as input we would use CnnPolicy\n",
        "model = PPO(\n",
        "    policy='MlpPolicy',\n",
        "    env=env,\n",
        "    n_steps=1024,\n",
        "    batch_size=64,\n",
        "    n_epochs=4,\n",
        "    gamma=0.999,\n",
        "    gae_lambda=0.98,\n",
        "    ent_coef=0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "6PHscs1UCfwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take time, do it on a GPU:"
      ],
      "metadata": {
        "id": "D4kr7ICMCi7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hqQ_etEy1N"
      },
      "outputs": [],
      "source": [
        "# train PPO agent\n",
        "model.learn(total_timesteps=int(1e6))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ppo-LunarLander-v2\" # A good name is {model_architecture}-{env_id}\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "zCYkpOaOCaeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push the model to the HF hub"
      ],
      "metadata": {
        "id": "xuxm5Nj0AWmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZiFBBlzxzxY"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2E--IJu8JYq"
      },
      "outputs": [],
      "source": [
        "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "package_to_hub(model=model,\n",
        "               model_name=model_name,\n",
        "               model_architecture=\"PPO\",\n",
        "               env_id=env_id,\n",
        "               eval_env=eval_env,\n",
        "               repo_id=f\"migolan/{model_name}\", # A good name is {username}/{model_architecture}-{env_id}\n",
        "               commit_message=\"Upload PPO LunarLander-v2 trained agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T79AEAWEFIxz"
      },
      "source": [
        "The script above should have displayed a link to a model repository such as https://huggingface.co/osanseviero/test_sb3. When you go to this link, you can:\n",
        "* See a video preview of your agent at the right.\n",
        "* Click \"Files and versions\" to see all the files in the repository.\n",
        "* Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
        "* A model card (`README.md` file) which gives a description of the model\n",
        "\n",
        "Compare the results of your LunarLander-v2 with your classmates using the leaderboard üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWnuQHRfFRa"
      },
      "source": [
        "# Load a saved model from the Hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy # conversion tool that will help us run the environment correctly https://github.com/Farama-Foundation/Shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "JoAvEznAg1Io",
        "outputId": "a0fe8835-dea6-4e79-d03a-cfa76c6a7b8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0a1 (from shimmy)\n",
            "  Using cached gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 0.28.1\n",
            "    Uninstalling gymnasium-0.28.1:\n",
            "      Successfully uninstalled gymnasium-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "stable-baselines3 2.0.0a5 requires gymnasium==0.28.1, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-1.0.0 shimmy-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gymnasium"
                ]
              },
              "id": "736139edd63b40bda2c165d33b4c2577"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oj8PSGHJfwz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d4f2a8-6130-4a73-c48d-fd777358c153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== CURRENT SYSTEM INFO ==\n",
            "- OS: Linux-6.1.85+-x86_64-with-glibc2.35 # 1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "- Python: 3.10.12\n",
            "- Stable-Baselines3: 2.0.0a5\n",
            "- PyTorch: 2.5.1+cu121\n",
            "- GPU Enabled: False\n",
            "- Numpy: 1.26.4\n",
            "- Cloudpickle: 3.1.0\n",
            "- Gymnasium: 0.28.1\n",
            "- OpenAI Gym: 0.25.2\n",
            "\n",
            "== SAVED MODEL SYSTEM INFO ==\n",
            "OS: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic #1 SMP Sun Apr 24 10:03:06 PDT 2022\n",
            "Python: 3.7.13\n",
            "Stable-Baselines3: 1.5.0\n",
            "PyTorch: 1.11.0+cu113\n",
            "GPU Enabled: True\n",
            "Numpy: 1.21.6\n",
            "Gym: 0.21.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:437: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  th_object = th.load(file_content, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# id this doesn't work amke sure you've installed shimmy\n",
        "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
        "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
        "# Go to https://huggingface.co/models?library=stable-baselines3 to see the list of all the Stable-baselines3 saved models.\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "\n",
        "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
        "# But Python 3.6, 3.7 use protocol 4\n",
        "# In order to get compatibility we need to:\n",
        "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
        "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
        "custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0Y-qgPgLUf"
      },
      "source": [
        "# Evaluate the agent\n",
        "When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new environment for evaluation, with a monitor\n",
        "eval_env = Monitor(gym.make(env_id, render_mode='rgb_array'))\n",
        "\n",
        "# Evaluate the model\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dsebs0dfhwr",
        "outputId": "bab8aac9-86ce-4453-bbb8-5cb9b6cf30d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=271.37 +/- 64.693128743265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "# Improve Agent\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here are some ideas to get to the top of the leaderboard:\n",
        "* Train more steps\n",
        "* Try different hyperparameters for `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.\n",
        "* Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) and try another model such as DQN.\n",
        "* **Push your new trained model** on the Hub üî•\n",
        "\n",
        "Other possible environments:\n",
        "* MountainCar-v0\n",
        "* CartPole-v1\n",
        "* CarRacing-v0\n",
        "\n",
        "Check how they work at https://www.gymlibrary.dev."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
